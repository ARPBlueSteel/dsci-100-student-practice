{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DSCI 100 - Introduction to Data Science\n",
    "\n",
    "\n",
    "### Lecture 7 - Classification II: Evaluating & Tuning\n",
    "\n",
    "<img src=\"img/intentional_arrival.png\" width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Housekeeping\n",
    "\n",
    "- Lots of grading completed + ongoing\n",
    "- Grades are being posted now. We will look into those who have a 0.\n",
    "- Re-grade request form will be send out towards end of semester.\n",
    "- Mid-term Check-in Survey\n",
    "- TA specific Surveys will come out soon\n",
    "- Nothing else major to report!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today: unanswered questions from last week\n",
    "\n",
    "1. Is our model any good? How do we evaluate it?\n",
    "\n",
    "2. How do we choose `k` in K-nearest neighbours classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "To add evaluation into our classification pipeline, we:\n",
    "\n",
    "1. Split our data into two subsets: *training data* and *testing data*.\n",
    "2. Build the model & choose K using training data only (sometimes called tuning)\n",
    "3. Compute accuracy by predicting labels on testing data only\n",
    "\n",
    "We'll now talk about each step individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://ubc-dsci.github.io/introduction-to-datascience/img/training_test.jpeg\" width=\"1100\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tuning and evaluating the Model\n",
    "\n",
    "<center>\n",
    "<img src=\"https://ubc-dsci.github.io/introduction-to-datascience/img/ML-paradigm-test.png\" width=\"1700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Golden Rule of Machine Learning / Statistics:** *Don't use your testing data to train your model!*\n",
    "\n",
    "## Why?\n",
    "\n",
    "Showing your classifier the labels of evaluation data is like cheating on a test; it'll look more accurate than it really is<br>\n",
    "\n",
    "- \"training your model\" includes choosing K, choosing predictors, choosing the model, scaling/centering variables, etc!\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img width=\"400px\" src=\"https://media2.giphy.com/media/12vJgj7zMN3jPy/giphy.gif\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Splitting Data\n",
    "\n",
    "There are two important things to do when splitting data.\n",
    "\n",
    "1. **Shuffling:** randomly reorder the data before splitting\n",
    "2. **Stratification:** make sure the two split subsets of data have roughly equal proportions of the different labels\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://ubc-dsci.github.io/introduction-to-datascience/img/training_test.jpeg\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "**Why?** \n",
    "\n",
    "(`tidymodels` thankfully automatically does both of these things)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing K (or, \"tuning'' the model)\n",
    "\n",
    "Want to choose K to maximize accuracy, but:\n",
    "- we can't use test data to evaluate accuracy (cheating!)\n",
    "- we can't use training data to evaluate accuracy (choosing K is part of training!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution:** Split the training data further into *training data* and *validation data sets*\n",
    "\n",
    "<br>2a. Choose some candidate values of K\n",
    "<br>2b. Split the **training data** into two sets - one called the **training set**, another called the **validation set**\n",
    "<br>2c. For each K, train the model using **training set only**\n",
    "<br>2d. Evaluate accuracy for each using **validation set only**\n",
    "<br>2e. Pick the K that maximizes validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*But what if we get a bad training set? Just by chance?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "We can get a better estimate of accuracy by splitting *multiple ways* and *averaging*\n",
    "\n",
    "<center>\n",
    "<img src=\"https://ubc-dsci.github.io/introduction-to-datascience/img/cv.png\" width=\"1100\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting & Overfitting\n",
    "\n",
    "\n",
    "**Overfitting:** when your model is too sensitive to your training data; noise can influence predictions!\n",
    "\n",
    "**Underfitting:** when your model isn't sensitive enough to training data; useful information is ignored!\n",
    "\n",
    "\n",
    "<center>\n",
    "<img width=\"800\" src=\"img/under_over_fitting.png\">\n",
    "</center>\n",
    "\n",
    "Source: http://kerckhoffs.schaathun.net/FPIA/Slides/09OF.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting & Overfitting\n",
    "**Which of these are under-, over-, and good fits?** \n",
    "\n",
    "<center>\n",
    "<img width=\"1200px\" src=\"https://ubc-dsci.github.io/introduction-to-datascience/_main_files/figure-html/06-decision-grid-K-1.png\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting & Overfitting\n",
    "For KNN: small K overfits, large K underfits, both cause lower accuracy\n",
    "\n",
    "<center>\n",
    "<img src=\"https://ubc-dsci.github.io/introduction-to-datascience/_main_files/figure-html/06-lots-of-ks-1.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Big Picture\n",
    "\n",
    "<center>\n",
    "<img align=\"left\" src=\"https://ubc-dsci.github.io/introduction-to-datascience/img/train-test-overview.jpeg\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Worksheet Time! Go for it!\n",
    "\n",
    "1. Go to your **project groups** \n",
    "2. You can work on the worksheet or discuss your group project proposal if needed \n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://memegenerator.net/img/instances/63959626/wont-you-be-my-k-nearest-neighbor.jpg\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Class Activity\n",
    "\n",
    "In your group, discuss the following prompts. \n",
    "- Explain what a test, validation and training data set are in your own words\n",
    "- Explain cross-validation in your own words\n",
    "- Imagine if we train *and* evaluate accuracy on all the data. **How can I get 100% accuracy, *always*?**\n",
    "- Why can't I use cross validation when testing?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.0"
  },
  "rise": {
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
