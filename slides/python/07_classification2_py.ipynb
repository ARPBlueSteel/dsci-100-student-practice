{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DSCI 100 - Introduction to Data Science\n",
    "\n",
    "\n",
    "### Lecture 7 - Classification II: Evaluating & Tuning\n",
    "\n",
    "<img src=\"https://datasciencebook.ca/_main_files/figure-html/06-decision-grid-K-1.png\" width=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Housekeeping\n",
    "\n",
    "- Midterm on Thursday at 12:30\n",
    "- Covers weeks 1-6\n",
    "    - Introduction to Python and Pandas ([Ch 1 in textbook](https://python.datasciencebook.ca/intro.html))\n",
    "    - Reading Data ([Ch 2](https://python.datasciencebook.ca/reading.html))\n",
    "    - Wrangling Data ([Ch 3](https://python.datasciencebook.ca/wrangling.html))\n",
    "    - Visualizing Data ([Ch 4](https://python.datasciencebook.ca/viz.html))\n",
    "    - Version Control ([Ch 12](https://python.datasciencebook.ca/version-control.html))\n",
    "    - Classification I: training & predicting ([Ch 5](https://python.datasciencebook.ca/classification1.html))\n",
    "- quiz is 70 minutes\n",
    "    - Multiple choice\n",
    "    - Short answer\n",
    "    - Fill in the blank coding questions\n",
    "- on Canvas with lockdown browser\n",
    "    - use Chrome or Firefox. Disable browser extensions\n",
    "    - You will be able to access the [Reference sheet for Python](https://ubc-dsci.github.io/dsci-100-student/REFERENCE_PYTHON.html)\n",
    "    - Please bring a calculator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today: unanswered questions from last week\n",
    "\n",
    "1. Is our model any good? How do we **evaluate** it?\n",
    "\n",
    "2. How do we choose `k` in K-nearest neighbours classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to measure classifier performance?\n",
    "</br>\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$Accuracy  = \\dfrac{\\#\\; correct\\; predictions}{\\#\\; total\\; predictions}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Downside: doesn't tell you the type of mistake being made\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confusion matrix\n",
    "</br>\n",
    "\n",
    "Here is an example of confusion matrix with cancer diagnosis data we've seen before.\n",
    "\n",
    "</br>\n",
    "\n",
    "<table>\n",
    "<thead style=\"font-size: 40px\";>\n",
    "<tr class=\"header\">\n",
    "<th></th>\n",
    "<th>Truly Malignant</th>\n",
    "<th>Truly Benign</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody style=\"font-size: 40px\";>\n",
    "<tr class=\"odd\">\n",
    "<td><strong>Predicted Malignant</strong></td>\n",
    "<td>1</td>\n",
    "<td>4</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td><strong>Predicted Benign</strong></td>\n",
    "<td>3</td>\n",
    "<td>57</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Typically we consider one of the class labels as \"positive\" - in this case the \"Malignant\" status is more interesting to researchers, hence we consider that label as \"positive\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this matrix, observations are sorted into the four cells based on its true class and predicted class. Each cell gives the total count of observations with a particular combination of true/predicted class. There's a tonne of information you can learn from this table. For eample, it shows us how often predictions are wrong, broken down by the predicted class. Or it shows us how likely that a person with malignant cancer doesn't get diagnosed correctly, etc.\n",
    "\n",
    "Downside: confusion matrix contains super useful information, but we want to summarize it in a way that is easy for making comparisons.\n",
    "To introduce summary measures of the confusion matrix in general, let us consider one class label as \"positive\" - usually the one considered more interesting, e.g. \"Maglignant\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Relabeling the above confusion matrix: \n",
    "\n",
    "</br>\n",
    "\n",
    "<table>\n",
    "<thead style=\"font-size: 40px\";>\n",
    "<tr class=\"header\">\n",
    "<th></th>\n",
    "<th>Truly Positive</th>\n",
    "<th>Truly Negative</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody style=\"font-size: 40px\";>\n",
    "<tr class=\"odd\">\n",
    "<td><strong>Predicted Positive</strong></td>\n",
    "<td>1</td>\n",
    "<td>4</td>\n",
    "</tr>\n",
    "<tr class=\"even\">\n",
    "<td><strong>Predicted Negative</strong></td>\n",
    "<td>3</td>\n",
    "<td>57</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</br>\n",
    "\n",
    "Note that:\n",
    "* Top left cell = # correct positive predictions.\n",
    "* Top *row* = # total positive predictions.\n",
    "* Left *column* = # truly positive observations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Precision and Recall\n",
    "\n",
    "\n",
    "$$\n",
    "{Precision}  = \\dfrac{{\\#\\; correct\\; positive\\; predictions}}{{\\#\\; total\\; positive\\; predictions}} \\quad\\quad\\quad \\quad {Recall}  = \\dfrac{{\\#\\; correct\\; positive\\; predictions}}{{\\#\\; total\\; truly\\; positive\\; observations}}\n",
    "$$\n",
    "\n",
    "</br>\n",
    "\n",
    "In the above confusion matrix, precision = 1/(1+4) and recall = 1/(1+3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Precision quantifies how many of the positive predictions the classifier made were actually positive. Intuitively, we would like a classifier to have a high precision: for a classifier with high precision, if the classifier reports that a new observation is positive, we can trust that the new observation is indeed positive. \n",
    "\n",
    "Recall quantifies how many of the positive observations in the test set were identified as positive. Intuitively, we would like a classifier to have a high recall: for a classifier with high recall, if there is a positive observation in the test data, we can trust that the classifier will find it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How good is good and which metric's more important?\n",
    "\n",
    "...is application context dependent. Use your judgement.\n",
    "\n",
    "</br>\n",
    "\n",
    "For example:\n",
    "- a 99% accuracy on cancer prediction may not be very useful. Why?\n",
    "- If we need patients with truly malignant cancer to be diagnosed correctly, what metric should we prioritize?\n",
    "- What if a classifier never guess positive except for the very few observations it is super confident in? What metric is affected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Ask students to discuss the three questions on the slides with their neighbor:\n",
    "\n",
    "- a 99% accuracy on cancer prediction may not be very useful. Why?\n",
    "    - Because malign cancer samples are much less common so a 99% accuracy might just mean that the classifier is alwayws predicting \"benign\", and if there are 99% \"benign\" cases in the training data, then the accuracy will also be 99%.\n",
    "- If we need patients with truly malignant cancer to be diagnosed correctly, what metric should we prioritize?\n",
    "    - The patients with truly malignant cancer are the \"total truly positive observations\" denominator in the \"Recall\" equation above. If we mostly care about these patients being correctly classified, we need to prioritize a high recall.\n",
    "- What if a classifier never guess positive except for the very few observations it is super confident in? What metric is affected?\n",
    "    - This will lead to fewer \"total positive predictions\" which is the denominator in the \"Precision\" equation, so the Precision score will be higher.\n",
    "    - It could also affect the number of \"correct positive predictions\", which would affect Precision, Recall, and Accuracy, but we don't have enough information in the prompt to tell for sure: this strategy could lead to  same number of correct positive predictions and just fewer incorrect positive predictions, or it could lead to fewer correct positive predictions as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adding evaluation to the pipeline for building a classifier\n",
    "\n",
    "To add evaluation into our classification pipeline, we:\n",
    "\n",
    "1. Split our data into two subsets: *training data* and *testing data*.\n",
    "2. Build the model & choose K using training data only (sometimes called tuning)\n",
    "3. Compute performance metrics (accuracy, precision, recall, etc.) by predicting labels on testing data only\n",
    "\n",
    "We'll now talk about each step individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<img src=\"https://python.datasciencebook.ca/_images/training_test.png\" width=\"1100\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tuning and evaluating the Model\n",
    "\n",
    "<center>\n",
    "<img src=\"https://python.datasciencebook.ca/_images/ML-paradigm-test.png\" width=\"1700\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Golden Rule of Machine Learning / Statistics:** *Don't use your testing data to train your model!*\n",
    "\n",
    "## Why?\n",
    "\n",
    "Showing your classifier the labels of evaluation data is like cheating on a test; it'll look more accurate than it really is<br>\n",
    "\n",
    "- \"training your model\" includes choosing K, choosing predictors, choosing the model, scaling/centering variables, etc!\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img width=\"400px\" src=\"https://media2.giphy.com/media/12vJgj7zMN3jPy/giphy.gif\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Splitting Data\n",
    "\n",
    "There are two important things to do when splitting data.\n",
    "\n",
    "1. **Shuffling:** randomly reorder the data before splitting\n",
    "2. **Stratification:** make sure the two split subsets of data have roughly equal proportions of the different labels\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://python.datasciencebook.ca/_images/training_test.png\" width=\"500\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "**Why?** \n",
    "\n",
    "(`sklearn` thankfully automatically does both of these things)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing K (or, \"tuning'' the model)\n",
    "\n",
    "Want to choose K to maximize accuracy, but:\n",
    "- we can't use test data to evaluate performance (cheating!)\n",
    "- we can't use training data to evaluate performance (choosing K is part of training!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution:** Split the training data further into *training data* and *validation data sets*\n",
    "\n",
    "<br>2a. Choose some candidate values of K\n",
    "<br>2b. Split the **training data** into two sets - one called the **training set**, another called the **validation set**\n",
    "<br>2c. For each K, train the model using **training set only**\n",
    "<br>2d. Evaluate accuracy (and/or other metrics of performance) for each using **validation set only**\n",
    "<br>2e. Pick the K that maximizes validation accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*But what if we get a bad training set? Just by chance?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-Validation\n",
    "\n",
    "We can get a better estimate of performance by splitting *multiple ways* and *averaging*\n",
    "\n",
    "<center>\n",
    "<img src=\"https://python.datasciencebook.ca/_images/cv.png\" width=\"1100\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting & Overfitting\n",
    "\n",
    "\n",
    "**Overfitting:** when your model is too sensitive to your training data; noise can influence predictions!\n",
    "\n",
    "**Underfitting:** when your model isn't sensitive enough to training data; useful information is ignored!\n",
    "\n",
    "\n",
    "<center>\n",
    "<img width=\"800\" src=\"img/under_over_fitting.png\">\n",
    "</center>\n",
    "\n",
    "Source: http://kerckhoffs.schaathun.net/FPIA/Slides/09OF.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting & Overfitting\n",
    "**Which of these are under-, over-, and good fits?** \n",
    "\n",
    "<center>\n",
    "<img width=\"1200px\" src=\"https://datasciencebook.ca/_main_files/figure-html/06-decision-grid-K-1.png\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Ask students to discuss with their neighbor.\n",
    "\n",
    "- K = 1 and K = 7 overfits as can been seen on the wiggly decision border.\n",
    "- K = 20 looks pretty decent\n",
    "- K = 300 underfits and misses many of the malignant observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting & Overfitting\n",
    "For KNN: small K overfits, large K underfits, both cause lower accuracy\n",
    "\n",
    "<center>\n",
    "<img src=\"https://datasciencebook.ca/_main_files/figure-html/06-lots-of-ks-1.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Big Picture\n",
    "\n",
    "<center>\n",
    "<img align=\"left\" src=\"https://python.datasciencebook.ca/_images/train-test-overview.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Worksheet Time! Go for it!\n",
    "\n",
    "1. Go to your **project groups** \n",
    "2. You can work on the worksheet or discuss your group project proposal if needed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Class Activity\n",
    "\n",
    "In your group, discuss the following prompts. \n",
    "- Explain what a test, validation and training data set are in your own words\n",
    "- Explain cross-validation in your own words\n",
    "- Imagine if we train *and* evaluate accuracy on all the data. **How can I get 100% accuracy, *always*?**\n",
    "- Why can't I use cross validation when testing?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "rise": {
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
